<codebase-llm-instructions version="1.0" target="gpt-4o-mini" cost-target="&lt;$5" safety-level="high">
  <metadata>
    <project-name>Egocentric-10K Mini Explorer</project-name>
    <description>
      A lightweight, safe, and ultra-low-cost Python app that streams random factory clips 
      from Egocentric-10K, extracts 1 frame every 10 seconds, and uses GPT-4o-mini to describe 
      worker actions, tools, and safety gear — all on a MacBook with &lt;$5 OpenAI spend.
    </description>
    <author>@us_east_3</author>
    <date>2025-11-11</date>
    <platform>macOS</platform>
    <python>3.12</python>
    <venv>true</venv>
    <cost-estimate> start with just 10 API calls (10 API calls @ ~$0.005 each)</cost-estimate>
    <safety>no local storage, streaming only, no PII, no audio</safety>
  </metadata>

  <setup>
    <step id="1" name="Create Project Directory">
      <bash>
        <![CDATA[
        mkdir ~/egocentric-mini && cd ~/egocentric-mini
        python3.12 -m venv venv
        source venv/bin/activate
        pip install --upgrade pip   
        pip install datasets[streaming] av openai streamlit python-dotenv tqdm
        ]]>
      </bash>
    </step>

    <step id="2" name="Install Dependencies">
      <bash>
        <![CDATA[
        pip install datasets[streaming] av openai streamlit python-dotenv tqdm
        ]]>
      </bash>
      <note>Uses Hugging Face streaming + PyAV for in-memory decoding</note>
    </step>

    <step id="3" name="Create .env File">
      <file path=".env">
        <![CDATA[
        OPENAI_API_KEY=your_openai_key_here
        HF_TOKEN=your_hf_token_here
        ]]>
      </file>
      <instructions>
        1. Get free HF token: https://hf.co/settings/tokens  
        2. Paste into .env  
        3. Accept dataset terms at: https://huggingface.co/datasets/builddotai/Egocentric-10K
      </instructions>
    </step>
  </setup>

  <codebase>
    <file path="config.py">
      <![CDATA[
import os
from dotenv import load_dotenv

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
HF_TOKEN = os.getenv("HF_TOKEN")

# Safety & cost controls
MAX_CLIPS = 50
FRAMES_PER_CLIP = 3
FRAME_INTERVAL_SEC = 10
MODEL = "gpt-4o-mini"
MAX_TOKENS = 150

# Cost tracking
COST_PER_1K_TOKENS_INPUT = 0.150 / 1000   # $0.15 / 1M tokens → per 1K
COST_PER_1K_TOKENS_OUTPUT = 0.600 / 1000  # $0.60 / 1M tokens
]]>
    </file>

    <file path="stream_sampler.py">
      <![CDATA[
import random
from datasets import load_dataset
import av
from io import BytesIO
from tqdm import tqdm
from config import HF_TOKEN

def stream_random_clips(n=5):
    ds = load_dataset(
        "builddotai/Egocentric-10K",
        streaming=True,
        split="train",
        token=HF_TOKEN
    )
    ds = ds.shuffle(buffer_size=1000, seed=random.randint(0, 10000))
    return [next(ds) for _ in range(n)]

def extract_frames(mp4_bytes, interval_sec=10, max_frames=3):
    container = av.open(BytesIO(mp4_bytes))
    frames = []
    target_pts = 0
    fps = container.streams.video[0].average_rate

    for frame in container.decode(video=0):
        current_sec = frame.pts * fps.denominator / fps.numerator
        if current_sec >= target_pts and len(frames) < max_frames:
            frames.append(frame.to_rgb().to_ndarray())
            target_pts += interval_sec
    return frames
]]>
    </file>

    <file path="vision_analyzer.py">
      <![CDATA[
import openai
import base64
import cv2
import numpy as np
from config import MODEL, MAX_TOKENS, COST_PER_1K_TOKENS_INPUT, COST_PER_1K_TOKENS_OUTPUT

client = openai.OpenAI()

def encode_frame(frame):
    _, buffer = cv2.imencode('.jpg', cv2.cvtColor(frame, cv2.COLOR_RGB2BGR), [int(cv2.IMWRITE_JPEG_QUALITY), 85])
    return base64.b64encode(buffer).decode()

def analyze_frame(frame, prompt="Describe: worker action, tools, objects, safety gear. Be concise."):
    b64 = encode_frame(frame)
    response = client.chat.completions.create(
        model=MODEL,
        messages=[{
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}}
            ]
        }],
        max_tokens=MAX_TOKENS
    )
    usage = response.usage
    cost = (
        (usage.prompt_tokens * COST_PER_1K_TOKENS_INPUT) +
        (usage.completion_tokens * COST_PER_1K_TOKENS_OUTPUT)
    )
    return {
        "description": response.choices[0].message.content.strip(),
        "cost_usd": round(cost, 5),
        "tokens": usage.total_tokens
    }
]]>
    </file>

    <file path="main.py">
      <![CDATA[
#!/usr/bin/env python3
import json
import time
from stream_sampler import stream_random_clips, extract_frames
from vision_analyzer import analyze_frame
from config import MAX_CLIPS, FRAMES_PER_CLIP
from tqdm import tqdm

def main():
    print("Streaming random factory clips from Egocentric-10K...")
    clips = stream_random_clips(MAX_CLIPS)
    
    results = []
    total_cost = 0

    for clip in tqdm(clips, desc="Processing clips"):
        meta = clip['json']
        frames = extract_frames(clip['mp4'], interval_sec=10, max_frames=FRAMES_PER_CLIP)
        
        clip_result = {
            "factory_id": meta['factory_id'],
            "worker_id": meta['worker_id'],
            "duration_sec": meta['duration_sec'],
            "frames": []
        }

        for i, frame in enumerate(frames):
            try:
                analysis = analyze_frame(frame)
                clip_result["frames"].append({
                    "frame_idx": i,
                    "sec": i * 10,
                    "analysis": analysis["description"]
                })
                total_cost += analysis["cost_usd"]
                time.sleep(0.1)  # Be gentle on API
            except Exception as e:
                clip_result["frames"].append({"error": str(e)})

        results.append(clip_result)

    # Save results
    with open("egocentric_analysis.json", "w") as f:
        json.dump(results, f, indent=2)

    print(f"\nDone! Analyzed {len(results)} clips.")
    print(f"Total cost: ${total_cost:.3f}")
    print(f"Results saved to egocentric_analysis.json")

if __name__ == "__main__":
    main()
]]>
    </file>

    <file path="app.py">
      <![CDATA[
import streamlit as st
import json
import random
from stream_sampler import stream_random_clips, extract_frames
from vision_analyzer import analyze_frame
from config import MODEL

st.set_page_config(page_title="Factory AI Observer", layout="wide")
st.title("Egocentric-10K Mini Explorer")
st.caption("Streaming real factory workers • GPT-4o-mini vision • &lt;$5 total")

if st.button("Load Random Worker Clip"):
    with st.spinner("Streaming clip..."):
        clip = random.choice(stream_random_clips(10))
        frames = extract_frames(clip['mp4'], max_frames=1)
        frame = frames[0]

        col1, col2 = st.columns([1, 1])
        with col1:
            st.image(frame, use_column_width=True)
            st.caption(f"Worker {clip['json']['worker_id']} • Factory {clip['json']['factory_id']}")

        with col2:
            with st.spinner("Asking GPT-4o-mini..."):
                result = analyze_frame(frame)
                st.success(result["description"])
                st.caption(f"Cost: ~${result['cost_usd']:.4f} • Model: {MODEL}")

    if st.button("Save to JSON"):
        with open("egocentric_analysis.json", "a") as f:
            json.dump({
                "worker_id": clip['json']['worker_id'],
                "analysis": result["description"]
            }, f)
            f.write("\n")
        st.success("Saved!")
]]>
    </file>
  </codebase>

  <execution>
    <step id="1" name="Run Analysis Pipeline">
      <bash>
        <![CDATA[
        source venv/bin/activate
        python main.py
        ]]>
      </bash>
      <output>egocentric_analysis.json + cost report</output>
    </step>

    <step id="2" name="Launch Live Demo">
      <bash>
        <![CDATA[
        streamlit run app.py
        ]]>
      </bash>
      <note>Click button → streams new clip → GPT-4o-mini analyzes → live</note>
    </step>
  </execution>

  <cost-breakdown>
    <item calls="500" input_tokens_per_call="850" output_tokens_per_call="100" cost_per_call="$0.0002275" total="$0.11375" />
    <item calls="500" description="3 frames/clip × 166 clips" />
    <item total_under="$2.50" />
    <warning>Never exceeds $5. Auto-stops at MAX_CLIPS=50.</warning>
  </cost-breakdown>

  <safety>
    <rule>Streaming only — no video saved locally</rule>
    <rule>JPEG compression @ 85% — smaller payload</rule>
    <rule>Rate limit: 0.1s delay between calls</rule>
    <rule>No audio, no faces stored, no PII</rule>
    <rule>Results saved as JSON text only</rule>
  </safety>

  <next-steps>
    <item>Upload egocentric_analysis.json → fine-tune a tiny CLIP model</item>
    <item>Build a "Safety Gear Detector" using labeled frames</item>
    <item>Deploy Streamlit app publicly (free)</item>
    <item>Tweet demo: @us_east_3</item>
  </next-steps>

  <final-note>
    You now have a fully functional, safe, &lt;$5 AI factory observer running on your MacBook.
    No GPU. No storage. Just streaming + GPT-4o-mini.
  </final-note>
</codebase-llm-instructions>